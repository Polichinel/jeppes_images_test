{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import os\n",
    "import re\n",
    "from xml.etree import ElementTree, ElementInclude\n",
    "from collections import Counter\n",
    "\n",
    "import iptcinfo3\n",
    "from iptcinfo3 import IPTCInfo\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root = '/home/simon/Documents/Bodies/data/jeppe/', transforms = None, n_obs = 100):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.n_obs = n_obs\n",
    "\n",
    "        # the selection need to happen here\n",
    "        self.classes = [''] + self.__get_classes__() # list of classes accroding to n_obs, see __get_classes__\n",
    "        self.classes_int = np.arange(0,len(self.classes)) # from 1 since no background '0'\n",
    "        self.boxes = self.__get_boxes__() # list of xml files (box info) to n_obs, see __get_classes__\n",
    "        self.imgs = [f\"{i.split('.')[0]}.jpg\" for i in self.boxes] # list of images - only take images with box info! and > n_obs\n",
    "             \n",
    "    def __get_classes__(self):\n",
    "        \"\"\"Creates a list of classes with >= n_obs observations\"\"\"\n",
    "        n_obs = self.n_obs\n",
    "        path = os.path.join(self.root, \"images\")\n",
    "\n",
    "        obj_name = []\n",
    "        classes = []\n",
    "\n",
    "        # Get all objects that have been annotated\n",
    "        for filename in os.listdir(path):\n",
    "            if filename.split('.')[1] == 'xml':\n",
    "                box_path = os.path.join(path, filename)\n",
    "\n",
    "                tree = ElementTree.parse(box_path)\n",
    "                lst_obj = tree.findall('object')\n",
    "\n",
    "                for j in lst_obj:\n",
    "                    obj_name.append(j.find('name').text)\n",
    "\n",
    "\n",
    "        # now, only keep the objects w/ >= n_obs observations\n",
    "        c = Counter(obj_name)\n",
    "\n",
    "        for i in c.items():\n",
    "            if i[1] >= n_obs:\n",
    "                classes.append(i[0])\n",
    "        \n",
    "        return(classes)\n",
    "\n",
    "    def __get_boxes__(self):\n",
    "        \"\"\"Make sure you only get images with valid boxes frrom the classes list - see __get_classes__\"\"\"\n",
    "\n",
    "        path = os.path.join(self.root, \"images\")\n",
    "\n",
    "        boxes = []\n",
    "        # Get all objects that have been annotated\n",
    "        for filename in os.listdir(path):\n",
    "            if filename.split('.')[1] == 'xml':\n",
    "                box_path = os.path.join(path, filename)\n",
    "\n",
    "                tree = ElementTree.parse(box_path)\n",
    "                lst_obj = tree.findall('object')\n",
    "\n",
    "                # If there is one or more objects from the classes list, save the box filename\n",
    "                if len(set([j.find('name').text for j in lst_obj]) & set(self.classes)) > 0:\n",
    "                    boxes.append(filename)\n",
    "\n",
    "        # Sort and return the boxes\n",
    "        boxes = sorted(boxes)\n",
    "        return(boxes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # dict to convert classes into classes_int\n",
    "        class_to_int = dict(zip(self.classes,self.classes_int))        \n",
    "\n",
    "        # load images\n",
    "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "        box_path = os.path.join(self.root, \"images\", self.boxes[idx])\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize img 800x800 --------------------------------------------\n",
    "        target_size = 800\n",
    "\n",
    "        y_orig_size = img.shape[0] # the original y shape\n",
    "        x_orig_size = img.shape[1] # the original x shape\n",
    "        y_scale = target_size/y_orig_size # scale factor for boxes\n",
    "        x_scale = target_size/x_orig_size # scale factor for boxes\n",
    "\n",
    "        img = cv2.resize(img, (target_size, target_size))\n",
    "        # ----------------------------------------------------------------\n",
    "\n",
    "        img = np.moveaxis(img, -1, 0) # move channels in front so h,w,c -> c,h,w\n",
    "        img = img / 255.0 # norm ot range 0-1. Might move out..\n",
    "        img = torch.Tensor(img)\n",
    "\n",
    "        # Open xml path \n",
    "        tree = ElementTree.parse(box_path)\n",
    "\n",
    "        lst_obj = tree.findall('object')\n",
    "\n",
    "        obj_name = []\n",
    "        obj_ids = []\n",
    "        boxes = []\n",
    "\n",
    "        for i in lst_obj:\n",
    "        # here you need to ignore classes w/ n > n_obs\n",
    "\n",
    "            obj_name_str = i.find('name').text\n",
    "            if obj_name_str in self.classes:\n",
    "\n",
    "                obj_name.append(obj_name_str) # get the actual class name\n",
    "                obj_ids.append(class_to_int[i.find('name').text]) # get the int associated with the class name\n",
    "                lst_box = i.findall('bndbox')\n",
    "\n",
    "                for j in lst_box:\n",
    "\n",
    "                    xmin = float(j.find('xmin').text) * x_scale # scale factor to fit resized image\n",
    "                    xmax = float(j.find('xmax').text) * x_scale\n",
    "                    ymin = float(j.find('ymin').text) * y_scale\n",
    "                    ymax = float(j.find('ymax').text) * y_scale\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        num_objs = len(obj_ids) # number of objects\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(obj_ids, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes \n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id \n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd \n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs) # right now you do not differentiate between annotated images and not annotated images... \n",
    "\n",
    "\n",
    "    def target_classes(self):\n",
    "        t_inst_classes = dict(zip(self.classes_int,self.classes)) # just a int to string dict\n",
    "        return(t_inst_classes)\n",
    "\n",
    "    def coco_classes(self):\n",
    "        inst_classes = [\n",
    "            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "            'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "            'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "            'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "            'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "            'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "            'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] # a \"ordered\" list of the coco categories\n",
    "        return(inst_classes) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "dataset = MyDataset(n_obs=500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print('length of dataset = ', len(dataset), '\\n')\n",
    "\n",
    "img, target = dataset[6]\n",
    "print(img.shape, '\\n',target)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of dataset =  769 \n",
      "\n",
      "torch.Size([3, 800, 800]) \n",
      " {'boxes': tensor([[367.0213,  53.4694, 539.0958, 387.3469],\n",
      "        [365.9575,  54.2857, 539.6277, 390.6122],\n",
      "        [363.2979,  50.2041, 541.2234, 394.2857],\n",
      "        [355.5851,  47.7551, 543.6170, 419.1837],\n",
      "        [ 12.7660,  18.3673, 180.8511, 316.7347],\n",
      "        [  9.3085,  17.5510, 189.6277, 338.3673],\n",
      "        [ 17.5532,  21.6327, 180.8511, 336.7347],\n",
      "        [  4.5213,  11.8367, 185.9043, 357.5510]]), 'labels': tensor([1, 2, 3, 4, 1, 2, 3, 4]), 'image_id': tensor([6]), 'area': tensor([57451.8086, 58409.9062, 61220.8984, 69840.4219, 50151.1055, 57849.3203,\n",
      "        51455.4883, 62706.6875]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "if 'b' in ['e','b']:\n",
    "    print('cool')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cool\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "list1 = ['a','g','c']\n",
    "list2 = ['b','j','f']\n",
    "\n",
    "len(set(list1) & set(list2)) > 0"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "sorted(list1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['a', 'c', 'g']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "list1 = np.array( ['a','b','c'])\n",
    "list2 = np.array(['b','a','f'])\n",
    "list1.any(list2)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot perform reduce with flexible type",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2c6d52615934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlist1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot perform reduce with flexible type"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dataset = MyDataset()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "dataset.__getitem__(199)[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 800, 800])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "dataset.__getitem__(np.random.randint(1,100))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[0.2627, 0.2549, 0.2627,  ..., 0.5529, 0.5529, 0.5333],\n",
       "          [0.2549, 0.2706, 0.2627,  ..., 0.5451, 0.5451, 0.5333],\n",
       "          [0.2588, 0.2510, 0.2588,  ..., 0.5373, 0.5529, 0.5451],\n",
       "          ...,\n",
       "          [0.1137, 0.1137, 0.1059,  ..., 0.4275, 0.4275, 0.4196],\n",
       "          [0.1294, 0.1176, 0.1216,  ..., 0.4235, 0.4235, 0.4157],\n",
       "          [0.1412, 0.1255, 0.1451,  ..., 0.4196, 0.4235, 0.4118]],\n",
       " \n",
       "         [[0.2078, 0.2118, 0.2275,  ..., 0.4980, 0.4980, 0.4784],\n",
       "          [0.2000, 0.2196, 0.2275,  ..., 0.4902, 0.4902, 0.4784],\n",
       "          [0.2000, 0.1961, 0.2157,  ..., 0.4824, 0.4980, 0.4902],\n",
       "          ...,\n",
       "          [0.2000, 0.2000, 0.1922,  ..., 0.3765, 0.3765, 0.3686],\n",
       "          [0.2039, 0.1961, 0.1961,  ..., 0.3725, 0.3725, 0.3647],\n",
       "          [0.2157, 0.2039, 0.2196,  ..., 0.3686, 0.3725, 0.3608]],\n",
       " \n",
       "         [[0.0941, 0.0941, 0.1059,  ..., 0.2510, 0.2510, 0.2314],\n",
       "          [0.0863, 0.1059, 0.1059,  ..., 0.2431, 0.2431, 0.2314],\n",
       "          [0.0863, 0.0824, 0.0980,  ..., 0.2353, 0.2510, 0.2431],\n",
       "          ...,\n",
       "          [0.2510, 0.2549, 0.2471,  ..., 0.1686, 0.1686, 0.1608],\n",
       "          [0.2588, 0.2471, 0.2510,  ..., 0.1647, 0.1647, 0.1569],\n",
       "          [0.2627, 0.2471, 0.2667,  ..., 0.1608, 0.1686, 0.1529]]]),\n",
       " {'boxes': tensor([[262.7660, 201.6327, 750.2659, 653.0612],\n",
       "          [260.6383, 195.1020, 750.2659, 657.9592],\n",
       "          [260.1064, 197.5510, 751.8617, 660.0000]]),\n",
       "  'labels': tensor([1, 2, 3]),\n",
       "  'image_id': tensor([70]),\n",
       "  'area': tensor([220071.4062, 226627.6250, 227411.7344]),\n",
       "  'iscrowd': tensor([0, 0, 0])})"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print('length of dataset = ', len(dataset), '\\n')\n",
    "img, target = dataset[78]\n",
    "print(img.shape, '\\n',target)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of dataset =  816 \n",
      "\n",
      "torch.Size([3, 800, 800]) \n",
      " {'boxes': tensor([[139.5626, 300.9063, 273.7574, 470.3928],\n",
      "        [138.5686, 301.8127, 274.3539, 476.4351],\n",
      "        [135.1889, 297.5831, 270.3777, 473.1118],\n",
      "        [137.3757, 303.6254, 272.7634, 468.8822],\n",
      "        [331.8091, 169.1843, 469.1849, 474.6223],\n",
      "        [331.8091, 177.6435, 468.1909, 474.6223],\n",
      "        [329.0258, 181.8731, 470.3777, 477.3414],\n",
      "        [325.6461, 175.2266, 469.1849, 479.7583]]), 'labels': tensor([1, 2, 3, 7, 1, 2, 3, 7]), 'image_id': tensor([78]), 'area': tensor([22744.2012, 23711.1504, 23729.5293, 22373.7363, 41959.7773, 40502.4805,\n",
      "        41765.0039, 43712.1016]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "test_img = cv2.imread('/home/simon/Documents/Bodies/data/jeppe/images/JS67.jpg')\n",
    "test_img.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1960, 3008, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dataset.classes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['',\n",
       " 'person',\n",
       " 'adult',\n",
       " 'male',\n",
       " 'firearm',\n",
       " 'female',\n",
       " 'religious_garment_female',\n",
       " 'uniformed',\n",
       " 'child',\n",
       " 'youth']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "new_classes = []\n",
    "new_classes.append('')\n",
    "new_classes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "boxes = []\n",
    "len(torch.as_tensor(boxes, dtype=torch.float32)) == 0"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "dataset = MyDataset()\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bcb3aabee2b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-213b0708cf90>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# dict to convert classes into classes_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mclass_to_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# load images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "dataset.__getitem__(np.random.randint(1,100))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0929dbb2138d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-213b0708cf90>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# dict to convert classes into classes_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mclass_to_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# load images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "dataset = MyDataset()\n",
    "['background'] + dataset.classes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['background',\n",
       " 'person',\n",
       " 'adult',\n",
       " 'male',\n",
       " 'firearm',\n",
       " 'female',\n",
       " 'religious_garment_female',\n",
       " 'uniformed',\n",
       " 'child',\n",
       " 'youth']"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "dataset.__getitem__(0)[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 1960, 3008])"
      ]
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cvTest():\n",
    "    # imageToPredict = cv2.imread(\"img.jpg\", 3)\n",
    "    imageToPredict = cv2.imread(\"49466033\\\\img.png \", 3)\n",
    "    print(imageToPredict.shape)\n",
    "\n",
    "    # Note: flipped comparing to your original code!\n",
    "    # x_ = imageToPredict.shape[0]\n",
    "    # y_ = imageToPredict.shape[1]\n",
    "    y_ = imageToPredict.shape[0]\n",
    "    x_ = imageToPredict.shape[1]\n",
    "\n",
    "    targetSize = 416\n",
    "    x_scale = targetSize / x_\n",
    "    y_scale = targetSize / y_\n",
    "    print(x_scale, y_scale)\n",
    "    img = cv2.resize(imageToPredict, (targetSize, targetSize));\n",
    "    print(img.shape)\n",
    "    img = np.array(img);\n",
    "\n",
    "    # original frame as named values\n",
    "    (origLeft, origTop, origRight, origBottom) = (160, 35, 555, 470)\n",
    "\n",
    "    x = int(np.round(origLeft * x_scale))\n",
    "    y = int(np.round(origTop * y_scale))\n",
    "    xmax = int(np.round(origRight * x_scale))\n",
    "    ymax = int(np.round(origBottom * y_scale))\n",
    "    # Box.drawBox([[1, 0, x, y, xmax, ymax]], img)\n",
    "    drawBox([[1, 0, x, y, xmax, ymax]], img)\n",
    "\n",
    "\n",
    "cvTest()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pytorch_env': conda)"
  },
  "interpreter": {
   "hash": "75c7d75b569136f34c43ea04f7afadbcc4da5fe88f8aa24f803d6345aeeda121"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}